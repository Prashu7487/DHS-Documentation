{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to DHS Project \u00b6 A federated learning platform with Hadoop/Spark backend Get Started { .md-button }","title":"Home"},{"location":"#welcome-to-dhs-project","text":"A federated learning platform with Hadoop/Spark backend Get Started { .md-button }","title":"Welcome to DHS Project"},{"location":"aws/","text":"AWS Integration \u00b6 AWS CLI Setup \u00b6 curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip sudo ./aws/install Credential Configuration \u00b6 aws configure # Follow prompts for AWS credentials S3 Integration \u00b6 <!-- Add to core-site.xml --> <property> <name>fs.s3a.impl</name> <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value> </property> !> Version Alert : Use AWS SDK 1.11.1026 with Hadoop 3.4.1 to avoid compatibility issues","title":"AWS Integration"},{"location":"aws/#aws-integration","text":"","title":"AWS Integration"},{"location":"aws/#aws-cli-setup","text":"curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip sudo ./aws/install","title":"AWS CLI Setup"},{"location":"aws/#credential-configuration","text":"aws configure # Follow prompts for AWS credentials","title":"Credential Configuration"},{"location":"aws/#s3-integration","text":"<!-- Add to core-site.xml --> <property> <name>fs.s3a.impl</name> <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value> </property> !> Version Alert : Use AWS SDK 1.11.1026 with Hadoop 3.4.1 to avoid compatibility issues","title":"S3 Integration"},{"location":"config-files/","text":"Configuration Files \u00b6 Hadoop Core Configuration \u00b6 etc/hadoop/core-site.xml : <configuration> <property> <name>fs.defaultFS</name> <value>hdfs://0.0.0.0:9000</value> </property> </configuration> Spark Defaults \u00b6 spark-defaults.conf : spark.master yarn spark.eventLog.enabled true spark.sql.shuffle.partitions 2000","title":"Configuration Files"},{"location":"config-files/#configuration-files","text":"","title":"Configuration Files"},{"location":"config-files/#hadoop-core-configuration","text":"etc/hadoop/core-site.xml : <configuration> <property> <name>fs.defaultFS</name> <value>hdfs://0.0.0.0:9000</value> </property> </configuration>","title":"Hadoop Core Configuration"},{"location":"config-files/#spark-defaults","text":"spark-defaults.conf : spark.master yarn spark.eventLog.enabled true spark.sql.shuffle.partitions 2000","title":"Spark Defaults"},{"location":"hadoop/","text":"Hadoop Cluster Setup \u00b6 Prerequisites \u00b6 Java 8 OpenJDK SSH configured between nodes Proper user permissions Installation Steps \u00b6 Download Hadoop 3.4.1: wget https://archive.apache.org/dist/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz tar -xzvf hadoop-3.4.1.tar.gz Set environment variables: export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 export HADOOP_HOME=~/hadoop/hadoop-3.4.1 export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin Verify installation: hadoop version !> Critical Warning : Never format NameNode after initial setup unless starting fresh. Backup metadata regularly.","title":"Hadoop Setup"},{"location":"hadoop/#hadoop-cluster-setup","text":"","title":"Hadoop Cluster Setup"},{"location":"hadoop/#prerequisites","text":"Java 8 OpenJDK SSH configured between nodes Proper user permissions","title":"Prerequisites"},{"location":"hadoop/#installation-steps","text":"Download Hadoop 3.4.1: wget https://archive.apache.org/dist/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz tar -xzvf hadoop-3.4.1.tar.gz Set environment variables: export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 export HADOOP_HOME=~/hadoop/hadoop-3.4.1 export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin Verify installation: hadoop version !> Critical Warning : Never format NameNode after initial setup unless starting fresh. Backup metadata regularly.","title":"Installation Steps"},{"location":"installation/","text":"Installation & Dependencies \u00b6 Frontend Requirements \u00b6 npm install @heroicons/react npm install d3 chart.js react-chartjs-2 Backend Requirements \u00b6 pip install fastapi hdfs pyspark uvicorn pandas pyarrow fastparquet python-dotenv \"uvicorn[standard]\"","title":"Installation"},{"location":"installation/#installation-dependencies","text":"","title":"Installation &amp; Dependencies"},{"location":"installation/#frontend-requirements","text":"npm install @heroicons/react npm install d3 chart.js react-chartjs-2","title":"Frontend Requirements"},{"location":"installation/#backend-requirements","text":"pip install fastapi hdfs pyspark uvicorn pandas pyarrow fastparquet python-dotenv \"uvicorn[standard]\"","title":"Backend Requirements"},{"location":"operations/","text":"Operational Guide \u00b6 Service Management \u00b6 Start all services: ./start-services.sh HDFS Operations \u00b6 Create directories: hdfs dfs -mkdir -p /user/cdis/{tmpuploads,uploads,processed} Spark Job Submission \u00b6 spark-submit --master yarn --deploy-mode client app.py","title":"Operations Guide"},{"location":"operations/#operational-guide","text":"","title":"Operational Guide"},{"location":"operations/#service-management","text":"Start all services: ./start-services.sh","title":"Service Management"},{"location":"operations/#hdfs-operations","text":"Create directories: hdfs dfs -mkdir -p /user/cdis/{tmpuploads,uploads,processed}","title":"HDFS Operations"},{"location":"operations/#spark-job-submission","text":"spark-submit --master yarn --deploy-mode client app.py","title":"Spark Job Submission"},{"location":"spark/","text":"Spark Setup \u00b6 Version Compatibility \u00b6 Use Spark 3.5.x with Hadoop 3.4.x Python 3.8+ required Installation \u00b6 wget https://dlcdn.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz tar -xvzf spark-3.5.5-bin-hadoop3.tgz -C ~/spark Environment Configuration \u00b6 export SPARK_HOME=~/spark/spark-3.5.5-bin-hadoop3 export PATH=$PATH:$SPARK_HOME/bin export PYSPARK_PYTHON=~/wslenv/bin/python Integration with Hadoop \u00b6 ln -s $HADOOP_CONF_DIR/core-site.xml $SPARK_HOME/conf/ ln -s $HADOOP_CONF_DIR/hdfs-site.xml $SPARK_HOME/conf/","title":"Spark Setup"},{"location":"spark/#spark-setup","text":"","title":"Spark Setup"},{"location":"spark/#version-compatibility","text":"Use Spark 3.5.x with Hadoop 3.4.x Python 3.8+ required","title":"Version Compatibility"},{"location":"spark/#installation","text":"wget https://dlcdn.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz tar -xvzf spark-3.5.5-bin-hadoop3.tgz -C ~/spark","title":"Installation"},{"location":"spark/#environment-configuration","text":"export SPARK_HOME=~/spark/spark-3.5.5-bin-hadoop3 export PATH=$PATH:$SPARK_HOME/bin export PYSPARK_PYTHON=~/wslenv/bin/python","title":"Environment Configuration"},{"location":"spark/#integration-with-hadoop","text":"ln -s $HADOOP_CONF_DIR/core-site.xml $SPARK_HOME/conf/ ln -s $HADOOP_CONF_DIR/hdfs-site.xml $SPARK_HOME/conf/","title":"Integration with Hadoop"}]}