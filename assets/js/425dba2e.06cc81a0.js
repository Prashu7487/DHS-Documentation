"use strict";(self.webpackChunkdhs_docs=self.webpackChunkdhs_docs||[]).push([[886],{7053:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>d,contentTitle:()=>i,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"setups/hadoop","title":"Hadoop Cluster Setup","description":"This page contains guide for single node hadoop setup","source":"@site/docs/setups/hadoop.md","sourceDirName":"setups","slug":"/setups/hadoop","permalink":"/DHS-Documentation/docs/setups/hadoop","draft":false,"unlisted":false,"editUrl":"https://github.com/Prashu7487/DHS-Documentation/edit/main/docs/setups/hadoop.md","tags":[],"version":"current","lastUpdatedBy":"Prashu7487","lastUpdatedAt":1744618600000,"frontMatter":{},"sidebar":"docs","previous":{"title":"Prerequisite Guide","permalink":"/DHS-Documentation/docs/setups/prerequisites"},"next":{"title":"Spark Cluster Setup","permalink":"/DHS-Documentation/docs/setups/spark"}}');var a=o(4848),s=o(8453);const t={},i="Hadoop Cluster Setup",d={},l=[{value:"Download Hadoop binary",id:"download-hadoop-binary",level:2},{value:"\ud83d\udce5 Install Hadoop 3.4.1",id:"-install-hadoop-341",level:2},{value:"\ud83c\udf0d Set Environment Variables",id:"-set-environment-variables",level:2},{value:"\ud83d\uddc2\ufe0f Create Directories",id:"\ufe0f-create-directories",level:2},{value:"\ud83d\udd11 Change the permission of hadoop directory",id:"-change-the-permission-of-hadoop-directory",level:2},{value:"\ud83d\udd27 Configure Hadoop for Single Node Setup (pseudo-distributed mode)",id:"-configure-hadoop-for-single-node-setup-pseudo-distributed-mode",level:2},{value:"Hadoop Configuration Files",id:"hadoop-configuration-files",level:3},{value:"1. Edit file hadoop-env.sh:",id:"1-edit-file-hadoop-envsh",level:3},{value:"2. Edit file core-site.xml:",id:"2-edit-file-core-sitexml",level:3},{value:"3. Edit file hdfs-site.xml:",id:"3-edit-file-hdfs-sitexml",level:3},{value:"4. Edit file mapred-site.xml:",id:"4-edit-file-mapred-sitexml",level:3},{value:"5. Edit file yarn-site.xml:",id:"5-edit-file-yarn-sitexml",level:3},{value:"6. Format namenode",id:"6-format-namenode",level:3},{value:"\ud83d\ude80 Starting hadoop daemons-",id:"-starting-hadoop-daemons-",level:2},{value:"\ud83d\udd0d Check the running services",id:"-check-the-running-services",level:2},{value:"\ud83d\udcc2 Create HDFS Folders to store real datasets",id:"-create-hdfs-folders-to-store-real-datasets",level:2},{value:"\ud83d\udcdd Sample commands for further use-",id:"-sample-commands-for-further-use-",level:2},{value:"\ud83c\udf10 Access hadoop web services-",id:"-access-hadoop-web-services-",level:2},{value:"\ud83d\udd17 Common Hadoop &amp; YARN Web UIs",id:"-common-hadoop--yarn-web-uis",level:3}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"hadoop-cluster-setup",children:"Hadoop Cluster Setup"})}),"\n",(0,a.jsxs)(n.p,{children:["This page contains guide for single node hadoop setup\r\nfor multi-node hadoop setup refer ",(0,a.jsx)(n.a,{href:"navigate-to-setups/multi-node-hadoop",children:"multi-node hadoop setup"})]}),"\n",(0,a.jsx)(n.h2,{id:"download-hadoop-binary",children:"Download Hadoop binary"}),"\n",(0,a.jsxs)(n.p,{children:["Visit the Hadoop releases page (",(0,a.jsx)(n.a,{href:"https://hadoop.apache.org/releases.html",children:"https://hadoop.apache.org/releases.html"}),") to find a download URL for choosen hadoop version (make sure that this is compatible with spark,pyspark version you're going to choose and it has stable aws-sdk jar to furthure integrate with aws services)"]}),"\n",(0,a.jsx)(n.p,{children:"In this guide, we're going with hadoop 3.4.1"}),"\n",(0,a.jsx)(n.h2,{id:"-install-hadoop-341",children:"\ud83d\udce5 Install Hadoop 3.4.1"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"wget https://archive.apache.org/dist/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz\r\n\r\n# make a directory for hadoop\r\nmkdir ~/hadoop\r\n\r\n# unzip binary package\r\ntar -xvzf hadoop-3.4.1.tar.gz -C ~/hadoop\n"})}),"\n",(0,a.jsx)(n.h2,{id:"-set-environment-variables",children:"\ud83c\udf0d Set Environment Variables"}),"\n",(0,a.jsxs)(n.p,{children:["Open your ",(0,a.jsx)(n.code,{children:"~/.bashrc"})," file for editing:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"nano ~/.bashrc\n"})}),"\n",(0,a.jsx)(n.p,{children:"Setup Hadoop and Java environment variables at the end of the .bashrc file as below and then save the bash file and close it."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Hadoop environment variables\r\n# ~ is replaced with $HOME, change if needed in future\r\n# match from your actual path\r\nexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\r\nexport HADOOP_HOME=$HOME/hadoop/hadoop-3.4.1\r\nexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\r\nexport HADOOP_MAPRED_HOME=$HADOOP_HOME\r\nexport HADOOP_COMMON_HOME=$HADOOP_HOME\r\nexport HADOOP_HDFS_HOME=$HADOOP_HOME\r\nexport YARN_HOME=$HADOOP_HOME\r\nexport YARN_CONF_DIR=$HADOOP_CONF_DIR\r\n\r\n# Add Hadoop paths to PATH\r\nexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\r\n\r\n# Ensure system paths are included\r\nexport PATH=/usr/bin:/bin:$PATH\r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HADOOP_HOME/lib/native\n"})}),"\n",(0,a.jsx)(n.p,{children:"Apply the changes immediately:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"source ~/.bashrc\n"})}),"\n",(0,a.jsx)(n.p,{children:"Verify the Hadoop installation by checking the version:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"hadoop version\n"})}),"\n",(0,a.jsx)(n.p,{children:"Expected output (sample):"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"Hadoop 3.4.1\r\nSource code repository https://github.com/apache/hadoop.git -r 4d7825309348956336b8f06a08322b78422849b1\r\nCompiled by mthakur on 2024-10-09T14:57Z\r\nCompiled on platform linux-x86_64\r\nCompiled with protoc 3.23.4\r\nFrom source with checksum 7292fe9dba5e2e44e3a9f763fce3e680\r\nThis command was run using /home/prashu/hadoop/hadoop-3.4.1/share/hadoop/common/hadoop-common-3.4.1.jar\n"})}),"\n",(0,a.jsx)(n.h2,{id:"\ufe0f-create-directories",children:"\ud83d\uddc2\ufe0f Create Directories"}),"\n",(0,a.jsx)(n.p,{children:"These directories will be used for storing logs, temporary files, and metadata."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"mkdir -p $HADOOP_HOME/logs\r\nmkdir -p ~/hadoop/{tmp,logs,data/{namenode,datanode}}\n"})}),"\n",(0,a.jsx)(n.h2,{id:"-change-the-permission-of-hadoop-directory",children:"\ud83d\udd11 Change the permission of hadoop directory"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"chmod -R 755 ~/hadoop\n"})}),"\n",(0,a.jsx)(n.h2,{id:"-configure-hadoop-for-single-node-setup-pseudo-distributed-mode",children:"\ud83d\udd27 Configure Hadoop for Single Node Setup (pseudo-distributed mode)"}),"\n",(0,a.jsxs)(n.p,{children:["official docs for single node setup: ",(0,a.jsx)(n.a,{href:"https://hadoop.apache.org/docs/r3.3.0/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_OperationPseudo-Distributed_Operation",children:"docs"})]}),"\n",(0,a.jsxs)(n.p,{children:["All the Hadoop configuration files should be located in the ",(0,a.jsx)(n.code,{children:"~/hadoop/hadoop-3.4.1/etc/hadoop"})," directory."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"cd ~/hadoop/hadoop-3.4.1/etc/hadoop\n"})}),"\n",(0,a.jsx)(n.p,{children:"\ud83d\udccc The configuration setup steps are as follows:"}),"\n",(0,a.jsx)(n.h3,{id:"hadoop-configuration-files",children:"Hadoop Configuration Files"}),"\n",(0,a.jsx)(n.p,{children:"\ud83d\udcdd Hadoop\u2019s Java configuration is driven by two types of important configuration files:"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Read-only default configuration"})," - core-default.xml, hdfs-default.xml, yarn-default.xml and mapred-default.xml.\r\n",(0,a.jsx)(n.strong,{children:"Site-specific configuration"})," - etc/hadoop/core-site.xml, etc/hadoop/hdfs-site.xml, etc/hadoop/yarn-site.xml and etc/hadoop/mapred-site.xml."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Note:"})," For a single-node cluster, make sure to use your actual private IP address directly in the configuration file. Avoid using variables from ",(0,a.jsx)(n.code,{children:"~/.bashrc"})," file, as daemon processes might run in a different environment and may not be able to interpret those variables."]}),"\n",(0,a.jsx)(n.h3,{id:"1-edit-file-hadoop-envsh",children:"1. Edit file hadoop-env.sh:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"nano hadoop-env.sh\n"})}),"\n",(0,a.jsx)(n.p,{children:"Set Java environment variable as,"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\r\nexport HADOOP_NICENESS=0\n"})}),"\n",(0,a.jsx)(n.h3,{id:"2-edit-file-core-sitexml",children:"2. Edit file core-site.xml:"}),"\n",(0,a.jsx)(n.p,{children:"this file informs the Hadoop daemon where NameNode runs in the cluster. It contains configuration settings of Hadoop core such as I/O settings that are common to HDFS & MapReduce."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"nano core-site.xml\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Set this properties in this file (these are basic for more specific setup visit official docs), ",(0,a.jsx)(n.strong,{children:"use your private IP and username."})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:"<configuration>\r\n  \x3c!-- HDFS Address --\x3e\r\n  <property>\r\n    <name>fs.defaultFS</name>\r\n    <value>hdfs://172.31.47.110:9000</value>\r\n  </property>\r\n\r\n  \x3c!-- Temp Directory: User-specific path --\x3e\r\n  <property>\r\n    <name>hadoop.tmp.dir</name>\r\n    <value>file:///home/ubuntu/hadoop/tmp</value>\r\n  </property>\r\n</configuration>\n"})}),"\n",(0,a.jsx)(n.h3,{id:"3-edit-file-hdfs-sitexml",children:"3. Edit file hdfs-site.xml:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"nano hdfs-site.xml\n"})}),"\n",(0,a.jsxs)(n.p,{children:["this file contains configuration settings of HDFS daemons (i.e. NameNode, DataNode, Secondary NameNode). It also includes the replication factor and block size of HDFS, again ",(0,a.jsx)(n.strong,{children:"use your username."})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:"  \x3c!-- NameNode Directory: User-specific --\x3e\r\n  <property>\r\n    <name>dfs.namenode.name.dir</name>\r\n    <value>file:///home/ubuntu/hadoop/data/namenode</value>\r\n  </property>\r\n\r\n  \x3c!-- DataNode Directory: User-specific --\x3e\r\n  <property>\r\n    <name>dfs.datanode.data.dir</name>\r\n    <value>file:///home/ubuntu/hadoop/data/datanode</value>\r\n  </property>\r\n\r\n  \x3c!-- Replication Factor --\x3e\r\n  <property>\r\n    <name>dfs.replication</name>\r\n    <value>1</value>\r\n  </property>\r\n\r\n  \x3c!-- Enable WebHDFS if want to connect using hdfs --\x3e\r\n  <property>\r\n    <name>dfs.webhdfs.enabled</name>\r\n    <value>true</value>\r\n  </property>\r\n</configuration>\n"})}),"\n",(0,a.jsx)(n.h3,{id:"4-edit-file-mapred-sitexml",children:"4. Edit file mapred-site.xml:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"nano mapred-site.xml\n"})}),"\n",(0,a.jsx)(n.p,{children:"this file contains configuration settings of MapReduce application like the number of JVM that can run in parallel, the size of the mapper and the reducer process, CPU cores available for a process, etc."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:"<configuration>\r\n    <property>\r\n        <name>mapreduce.framework.name</name>\r\n        <value>yarn</value>\r\n    </property>\r\n    <property>\r\n        <name>mapreduce.application.classpath</name>\r\n        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>\r\n    </property>\r\n</configuration>\n"})}),"\n",(0,a.jsx)(n.h3,{id:"5-edit-file-yarn-sitexml",children:"5. Edit file yarn-site.xml:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"nano yarn-site.xml\n"})}),"\n",(0,a.jsx)(n.p,{children:"this file contains configuration settings of ResourceManager and NodeManager like application memory management size, the operation needed on program & algorithm, etc."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:"<configuration>\r\n    <property>\r\n        <name>yarn.nodemanager.aux-services</name>\r\n        <value>mapreduce_shuffle</value>\r\n    </property>\r\n    <property>\r\n        <name>yarn.nodemanager.env-whitelist</name>\r\n        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>\r\n    </property>\r\n    \x3c!-- The below setup is for m5.2xlarge, configure based on your system--\x3e\r\n    \x3c!-- Total resources YARN can use, spark-default resources must fit into this --\x3e\r\n    <property>\r\n        <name>yarn.nodemanager.resource.memory-mb</name>\r\n        <value>24576</value> \x3c!-- 24GB (leave other for OS/Hadoop--\x3e\r\n    </property>\r\n    <property>\r\n        <name>yarn.nodemanager.resource.cpu-vcores</name>\r\n        <value>7</value> \x3c!-- Leave 1 core for OS/HDFS --\x3e\r\n    </property>\r\n    <property>\r\n        <name>yarn.scheduler.maximum-allocation-mb</name>\r\n        <value>24576</value>\r\n    </property>\r\n</configuration>\n"})}),"\n",(0,a.jsx)(n.h3,{id:"6-format-namenode",children:"6. Format namenode"}),"\n",(0,a.jsx)(n.p,{children:"Go to the Hadoop home directory and format the Hadoop namenode, This formats the HDFS via the NameNode. Formatting the file system means initializing the directory specified by the dfs.name.dir variable."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Caution"}),": By formatting, You will lose all your data stored in the HDFS, and the jar files that you'll copy during aws setup, after each formatting you need to copy it again.\r\n",(0,a.jsx)(n.strong,{children:"Note:"})," Hadoop services should be running before formatting the Namenode."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"cd ~/hadoop/hadoop-3.4.1\r\nbin/hdfs namenode -format\n"})}),"\n",(0,a.jsx)(n.h2,{id:"-starting-hadoop-daemons-",children:"\ud83d\ude80 Starting hadoop daemons-"}),"\n",(0,a.jsx)(n.p,{children:"The NameNode is the centerpiece of an HDFS file system. It keeps the directory tree of all files stored in the HDFS and tracks all the files stored across the cluster."}),"\n",(0,a.jsx)(n.p,{children:"On startup, a DataNode connects to the Namenode and it responds to the requests from the Namenode for different operations."}),"\n",(0,a.jsx)(n.p,{children:"These are the commands to start all hadoop daemons at once, however you can always choose to start specific services by executing their respective executables."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Note:"})," We have given scripts to start/stop all hadoop and spark services at once from home directory in the additional section."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:" ~/hadoop/hadoop-3.4.1/sbin/start-all.sh\r\n ~/hadoop/hadoop-3.4.1/sbin/stop-all.sh\n"})}),"\n",(0,a.jsx)(n.h2,{id:"-check-the-running-services",children:"\ud83d\udd0d Check the running services"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"jps\n"})}),"\n",(0,a.jsx)(n.h2,{id:"-create-hdfs-folders-to-store-real-datasets",children:"\ud83d\udcc2 Create HDFS Folders to store real datasets"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# you can check username by `whoami` command\r\nhdfs dfs -mkdir /user\r\nhdfs dfs -mkdir /user/ubuntu\r\nhdfs dfs -mkdir /user/ubuntu/{tmpuploads, uploads, processed, tmpmerged}\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Note:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Upload datasets only to the tmpuploads folder."}),"\n",(0,a.jsx)(n.li,{children:"The uploads, processed, and tmpmerged directories should be used programmatically only to avoid inconsistencies and internal failures."}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"-sample-commands-for-further-use-",children:"\ud83d\udcdd Sample commands for further use-"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'# these commands are for hadoop on wsl,change accordingly\r\n# these commands assume that dataset is on localhost and you\'re moving it to hdfs, for remote hadoop first copy the data into remote machine then move it to hdfs\r\n\r\n# list the datasets\r\nhdfs dfs -ls /user/ubuntu/processed\r\n\r\n# upload the dataset\r\nhdfs dfs -put "path/to/file" /user/ubuntu/tmpuploads/filename.csv\r\n\r\n# remove a directory or file (-r for recursive)\r\nhdfs dfs -rm -r /path/to/remove\n'})}),"\n",(0,a.jsx)(n.h2,{id:"-access-hadoop-web-services-",children:"\ud83c\udf10 Access hadoop web services-"}),"\n",(0,a.jsxs)(n.p,{children:["\u26a0\ufe0f ",(0,a.jsx)(n.strong,{children:"Important:"})," Web UI Links May Vary by Port & Access"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["For a deployed cluster, replace ",(0,a.jsx)(n.code,{children:"localhost"})," with your public IP address and ensure the required ports are open in your security group."]}),"\n",(0,a.jsxs)(n.li,{children:["Alternatively, set up an SSH tunnel from your local machine to the server (see script in the ",(0,a.jsx)(n.code,{children:"additional-setup"})," section) to make the following links work properly."]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"-common-hadoop--yarn-web-uis",children:"\ud83d\udd17 Common Hadoop & YARN Web UIs"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"YARN ResourceManager"}),": ",(0,a.jsx)(n.a,{href:"http://localhost:8088/cluster",children:"http://localhost:8088/cluster"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"NodeManager"}),": ",(0,a.jsx)(n.a,{href:"http://localhost:8042/node/",children:"http://localhost:8042/node/"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Hadoop NameNode"}),": ",(0,a.jsx)(n.a,{href:"http://localhost:9870/",children:"http://localhost:9870/"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Hadoop DataNodes"}),": ",(0,a.jsx)(n.a,{href:"http://localhost:9864/datanode.html",children:"http://localhost:9864/datanode.html"})]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"You can use these web UIs to monitor and debug most aspects of the system. However, for programmatic access or command-line inspection, the following commands can be useful:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# List all YARN nodes with detailed information\r\nyarn node -list -showDetails\r\n\r\n# Monitor YARN resource usage in real-time\r\nyarn top\r\n\r\n# View the latest 50 lines of the log\r\n# used tail -n 50 for last 50 lines\r\ntail -f $HADOOP_HOME/logs/hadoop*-namenode-*.log\r\ntail -f $HADOOP_HOME/logs/hadoop*-datanode-*.log\r\ntail -f $HADOOP_HOME/logs/hadoop*-nodemanager-*.log\r\ntail -f $HADOOP_HOME/logs/hadoop*-resourcemanager-*.log\r\ntail -f 50 $HADOOP_HOME/logs/hadoop*-secondarynamenode-*.log\r\n\r\n# Check the status of all running YARN applications\r\nyarn application -list\n"})}),"\n",(0,a.jsx)(n.hr,{})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>t,x:()=>i});var r=o(6540);const a={},s=r.createContext(a);function t(e){const n=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);