"use strict";(self.webpackChunkdhs_docs=self.webpackChunkdhs_docs||[]).push([[886],{7053:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>d,contentTitle:()=>i,default:()=>h,frontMatter:()=>t,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"setups/hadoop","title":"Hadoop Cluster Setup","description":"This page contains guide for single node hadoop setup","source":"@site/docs/setups/hadoop.md","sourceDirName":"setups","slug":"/setups/hadoop","permalink":"/DHS-Documentation/docs/setups/hadoop","draft":false,"unlisted":false,"editUrl":"https://github.com/Prashu7487/DHS-Documentation/edit/main/docs/setups/hadoop.md","tags":[],"version":"current","lastUpdatedBy":"Prashu7487","lastUpdatedAt":1744618600000,"frontMatter":{},"sidebar":"docs","previous":{"title":"Prerequisite Guide","permalink":"/DHS-Documentation/docs/setups/prerequisites"},"next":{"title":"Spark Cluster Setup","permalink":"/DHS-Documentation/docs/setups/spark"}}');var r=n(4848),s=n(8453);const t={},i="Hadoop Cluster Setup",d={},l=[{value:"Download Hadoop binary",id:"download-hadoop-binary",level:2},{value:"\ud83d\udce5 Install Hadoop 3.4.1",id:"-install-hadoop-341",level:2},{value:"\ud83c\udf0d Set Environment Variables",id:"-set-environment-variables",level:2},{value:"\ud83d\uddc2\ufe0f Create Directories",id:"\ufe0f-create-directories",level:2},{value:"\ud83d\udd27 Configure Hadoop for Single Node Setup (pseudo-distributed mode)",id:"-configure-hadoop-for-single-node-setup-pseudo-distributed-mode",level:2},{value:"Hadoop Configuration Files",id:"hadoop-configuration-files",level:3},{value:"1. Edit file hadoop-env.sh:",id:"1-edit-file-hadoop-envsh",level:3},{value:"2. Edit file core-site.xml:",id:"2-edit-file-core-sitexml",level:3},{value:"3. Edit file hdfs-site.xml:",id:"3-edit-file-hdfs-sitexml",level:3},{value:"4. Edit file mapred-site.xml:",id:"4-edit-file-mapred-sitexml",level:3},{value:"5. Edit file yarn-site.xml:",id:"5-edit-file-yarn-sitexml",level:3},{value:"6. Format namenode",id:"6-format-namenode",level:3},{value:"\ud83d\ude80 Starting hadoop daemons-",id:"-starting-hadoop-daemons-",level:2},{value:"\ud83d\udd0d Check the running services",id:"-check-the-running-services",level:2},{value:"\ud83d\udcc2 Create HDFS Folders to store real datasets",id:"-create-hdfs-folders-to-store-real-datasets",level:2},{value:"\ud83d\udcdd Sample commands for further use-",id:"-sample-commands-for-further-use-",level:2},{value:"\ud83c\udf10 Access hadoop web services-",id:"-access-hadoop-web-services-",level:2}];function c(e){const a={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(a.header,{children:(0,r.jsx)(a.h1,{id:"hadoop-cluster-setup",children:"Hadoop Cluster Setup"})}),"\n",(0,r.jsxs)(a.p,{children:["This page contains guide for single node hadoop setup\r\nfor multi-node hadoop setup refer ",(0,r.jsx)(a.a,{href:"navigate-to-setups/multi-node-hadoop",children:"multi-node hadoop setup"})]}),"\n",(0,r.jsx)(a.h2,{id:"download-hadoop-binary",children:"Download Hadoop binary"}),"\n",(0,r.jsxs)(a.p,{children:["Visit the Hadoop releases page (",(0,r.jsx)(a.a,{href:"https://hadoop.apache.org/releases.html",children:"https://hadoop.apache.org/releases.html"}),") to find a download URL for choosen hadoop version (make sure that this is compatible with spark,pyspark version you're going to choose and it has stable aws-sdk jar to furthure integrate with aws services)"]}),"\n",(0,r.jsx)(a.p,{children:"In this guide, we're going with hadoop 3.4.1"}),"\n",(0,r.jsx)(a.h2,{id:"-install-hadoop-341",children:"\ud83d\udce5 Install Hadoop 3.4.1"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"wget https://archive.apache.org/dist/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz\r\n\r\n# make a directory for hadoop\r\nmkdir ~/hadoop\r\n\r\n# unzip binary package\r\ntar -xvzf hadoop-3.4.1.tar.gz -C ~/hadoop\n"})}),"\n",(0,r.jsx)(a.h2,{id:"-set-environment-variables",children:"\ud83c\udf0d Set Environment Variables"}),"\n",(0,r.jsxs)(a.p,{children:["Open your ",(0,r.jsx)(a.code,{children:"~/.bashrc"})," file for editing:"]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"nano ~/.bashrc\n"})}),"\n",(0,r.jsx)(a.p,{children:"Setup Hadoop and Java environment variables at the end of the .bashrc file as below and then save the bash file and close it."}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"# Hadoop environment variables\r\n# ~ is replaced with $HOME, change if needed in future\r\nexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\r\nexport HADOOP_HOME=$HOME/hadoop/hadoop-3.4.1\r\nexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\r\nexport HADOOP_MAPRED_HOME=$HADOOP_HOME\r\nexport HADOOP_COMMON_HOME=$HADOOP_HOME\r\nexport HADOOP_HDFS_HOME=$HADOOP_HOME\r\nexport YARN_HOME=$HADOOP_HOME\r\nexport YARN_CONF_DIR=$HADOOP_CONF_DIR\r\n\r\n# Add Hadoop paths to PATH\r\nexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\r\n\r\n# Ensure system paths are included\r\nexport PATH=/usr/bin:/bin:$PATH\r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HADOOP_HOME/lib/native\n"})}),"\n",(0,r.jsx)(a.p,{children:"Apply the changes immediately:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"source ~/.bashrc\n"})}),"\n",(0,r.jsx)(a.p,{children:"Verify the Hadoop installation by checking the version:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"hadoop version\n"})}),"\n",(0,r.jsx)(a.p,{children:"Expected output (sample):"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"Hadoop 3.4.1\r\nSource code repository https://github.com/apache/hadoop.git -r 4d7825309348956336b8f06a08322b78422849b1\r\nCompiled by mthakur on 2024-10-09T14:57Z\r\nCompiled on platform linux-x86_64\r\nCompiled with protoc 3.23.4\r\nFrom source with checksum 7292fe9dba5e2e44e3a9f763fce3e680\r\nThis command was run using /home/prashu/hadoop/hadoop-3.4.1/share/hadoop/common/hadoop-common-3.4.1.jar\n"})}),"\n",(0,r.jsx)(a.h2,{id:"\ufe0f-create-directories",children:"\ud83d\uddc2\ufe0f Create Directories"}),"\n",(0,r.jsx)(a.p,{children:"These directories will be used for storing logs, temporary files, and metadata."}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"mkdir -p $HADOOP_HOME/logs\r\nmkdir -p $HADOOP_HOME/tmp\r\nmkdir -p $HADOOP_HOME/data\r\nmkdir -p $HADOOP_HOME/data/namenode\r\nmkdir -p $HADOOP_HOME/data/datanode\n"})}),"\n",(0,r.jsx)(a.h2,{id:"-configure-hadoop-for-single-node-setup-pseudo-distributed-mode",children:"\ud83d\udd27 Configure Hadoop for Single Node Setup (pseudo-distributed mode)"}),"\n",(0,r.jsxs)(a.p,{children:["official docs for single node setup: ",(0,r.jsx)(a.a,{href:"https://hadoop.apache.org/docs/r3.3.0/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_OperationPseudo-Distributed_Operation",children:"docs"})]}),"\n",(0,r.jsxs)(a.p,{children:["All the Hadoop configuration files should be located in the ",(0,r.jsx)(a.code,{children:"~/hadoop/hadoop-3.4.1/etc/hadoop"})," directory."]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"cd ~/hadoop/hadoop-3.4.1/etc/hadoop\n"})}),"\n",(0,r.jsx)(a.p,{children:"\ud83d\udccc The configuration setup steps are as follows:"}),"\n",(0,r.jsx)(a.h3,{id:"hadoop-configuration-files",children:"Hadoop Configuration Files"}),"\n",(0,r.jsx)(a.p,{children:"\ud83d\udcdd Hadoop\u2019s Java configuration is driven by two types of important configuration files:"}),"\n",(0,r.jsxs)(a.p,{children:[(0,r.jsx)(a.strong,{children:"Read-only default configuration"})," - core-default.xml, hdfs-default.xml, yarn-default.xml and mapred-default.xml.\r\n",(0,r.jsx)(a.strong,{children:"Site-specific configuration"})," - etc/hadoop/core-site.xml, etc/hadoop/hdfs-site.xml, etc/hadoop/yarn-site.xml and etc/hadoop/mapred-site.xml."]}),"\n",(0,r.jsx)(a.h3,{id:"1-edit-file-hadoop-envsh",children:"1. Edit file hadoop-env.sh:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"nano hadoop-env.sh\n"})}),"\n",(0,r.jsx)(a.p,{children:"Set Java environment variable as,"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n"})}),"\n",(0,r.jsx)(a.h3,{id:"2-edit-file-core-sitexml",children:"2. Edit file core-site.xml:"}),"\n",(0,r.jsx)(a.p,{children:"this file informs the Hadoop daemon where NameNode runs in the cluster. It contains configuration settings of Hadoop core such as I/O settings that are common to HDFS & MapReduce."}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"nano core-site.xml\n"})}),"\n",(0,r.jsx)(a.p,{children:"Set this properties in this file (these are basic for more specific setup visit official docs)"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-xml",children:"<configuration>\r\n    \x3c!-- Default file system for HDFS --\x3e\r\n    <property>\r\n        <name>fs.defaultFS</name>\r\n        <value>hdfs://0.0.0.0:9000</value>\r\n    </property>\r\n\r\n    \x3c!-- Temporary directory for Hadoop --\x3e\r\n    <property>\r\n        <name>hadoop.tmp.dir</name>\r\n        <value>file:///home/prashu/hadoop/hadoop-3.4.1/tmp</value>\r\n    </property>\r\n</configuration>\n"})}),"\n",(0,r.jsx)(a.h3,{id:"3-edit-file-hdfs-sitexml",children:"3. Edit file hdfs-site.xml:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"nano hdfs-site.xml\n"})}),"\n",(0,r.jsx)(a.p,{children:"this file contains configuration settings of HDFS daemons (i.e. NameNode, DataNode, Secondary NameNode). It also includes the replication factor and block size of HDFS."}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-xml",children:"<configuration>\r\n    \x3c!-- Replication factor for single-node setup --\x3e\r\n    <property>\r\n        <name>dfs.replication</name>\r\n        <value>1</value>\r\n    </property>\r\n\r\n    <property>\r\n      <name>dfs.webhdfs.enabled</name>\r\n      <value>true</value>\r\n    </property>\r\n\r\n    \x3c!-- Directory for NameNode metadata, must otherwise you have to format the namenode after each startup --\x3e\r\n    <property>\r\n        <name>dfs.namenode.name.dir</name>\r\n        <value>file:///home/prashu/hadoop/hadoop-3.4.1/data/namenode</value>\r\n    </property>\r\n\r\n    \x3c!-- Directory for DataNode blocks --\x3e\r\n    <property>\r\n        <name>dfs.datanode.data.dir</name>\r\n        <value>file:///home/prashu/hadoop/hadoop-3.4.1/data/datanode</value>\r\n    </property>\r\n</configuration>\n"})}),"\n",(0,r.jsx)(a.h3,{id:"4-edit-file-mapred-sitexml",children:"4. Edit file mapred-site.xml:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"nano mapred-site.xml\n"})}),"\n",(0,r.jsx)(a.p,{children:"this file contains configuration settings of MapReduce application like the number of JVM that can run in parallel, the size of the mapper and the reducer process, CPU cores available for a process, etc."}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-xml",children:"<configuration>\r\n    <property>\r\n        <name>mapreduce.framework.name</name>\r\n        <value>yarn</value>\r\n    </property>\r\n    <property>\r\n        <name>mapreduce.application.classpath</name>\r\n        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>\r\n    </property>\r\n</configuration>\n"})}),"\n",(0,r.jsx)(a.h3,{id:"5-edit-file-yarn-sitexml",children:"5. Edit file yarn-site.xml:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"nano yarn-site.xml\n"})}),"\n",(0,r.jsx)(a.p,{children:"this file contains configuration settings of ResourceManager and NodeManager like application memory management size, the operation needed on program & algorithm, etc."}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-xml",children:"<configuration>\r\n    <property>\r\n        <name>yarn.nodemanager.aux-services</name>\r\n        <value>mapreduce_shuffle</value>\r\n    </property>\r\n    <property>\r\n        <name>yarn.nodemanager.env-whitelist</name>\r\n        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>\r\n    </property>\r\n</configuration>\n"})}),"\n",(0,r.jsx)(a.h3,{id:"6-format-namenode",children:"6. Format namenode"}),"\n",(0,r.jsx)(a.p,{children:"Go to the Hadoop home directory and format the Hadoop namenode, This formats the HDFS via the NameNode. Formatting the file system means initializing the directory specified by the dfs.name.dir variable."}),"\n",(0,r.jsx)(a.p,{children:"Caution: By formatting, You will lose all your data stored in the HDFS."}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"cd ~/hadoop/hadoop-3.4.1\r\nbin/hdfs namenode -format\n"})}),"\n",(0,r.jsx)(a.h2,{id:"-starting-hadoop-daemons-",children:"\ud83d\ude80 Starting hadoop daemons-"}),"\n",(0,r.jsx)(a.p,{children:"The NameNode is the centerpiece of an HDFS file system. It keeps the directory tree of all files stored in the HDFS and tracks all the files stored across the cluster."}),"\n",(0,r.jsx)(a.p,{children:"On startup, a DataNode connects to the Namenode and it responds to the requests from the Namenode for different operations."}),"\n",(0,r.jsx)(a.p,{children:"these are the commands to start all hadoop deamons at once, however you can always choose to start specific services by executing their respective executables."}),"\n",(0,r.jsxs)(a.p,{children:[(0,r.jsx)(a.strong,{children:"Note:"})," we have given an script to start all hadoop and spark services at once from home directory in the additional section."]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:" ~/hadoop/hadoop-3.4.1/sbin/start-all.sh\r\n ~/hadoop/hadoop-3.4.1/sbin/stop-all.sh\n"})}),"\n",(0,r.jsx)(a.h2,{id:"-check-the-running-services",children:"\ud83d\udd0d Check the running services"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"jps\n"})}),"\n",(0,r.jsx)(a.h2,{id:"-create-hdfs-folders-to-store-real-datasets",children:"\ud83d\udcc2 Create HDFS Folders to store real datasets"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"# you can check username ny `whoami` command\r\nhdfs dfs -mkdir /user\r\nhdfs dfs -mkdir /user/<user-name>/{tmpuploads,uploads,processed}\n"})}),"\n",(0,r.jsx)(a.p,{children:(0,r.jsx)(a.strong,{children:"Note:"})}),"\n",(0,r.jsxs)(a.ul,{children:["\n",(0,r.jsx)(a.li,{children:"Upload datasets only to the tmpuploads folder."}),"\n",(0,r.jsx)(a.li,{children:"The uploads and processed directories should be used programmatically only to avoid inconsistencies and internal failures."}),"\n"]}),"\n",(0,r.jsx)(a.h2,{id:"-sample-commands-for-further-use-",children:"\ud83d\udcdd Sample commands for further use-"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"# these commands are for hadoop on wsl,change accordingly\r\n# these commands assume that dataset is on localhost and you're moving it to hdfs, for remote hadoop first copy the data into remote machine then move it to hdfs\r\n\r\n# list the datasets\r\nhdfs dfs -ls /user/cdis/processed\r\n\r\n# upload the dataset\r\nhdfs dfs -put /mnt/d/projects/datasets/filename.csv /user/username/tmpuploads/filename.csv\r\n\r\n# remove a directory or file (-r for recursive)\r\nhdfs dfs -rm -r /path/to/remove\n"})}),"\n",(0,r.jsx)(a.h2,{id:"-access-hadoop-web-services-",children:"\ud83c\udf10 Access hadoop web services-"}),"\n",(0,r.jsxs)(a.p,{children:[(0,r.jsx)(a.strong,{children:"WARNING:"})," links may change as per port availability, and use your public IP in place of localhost for deployed cluster."]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"YARN ResourceManager: http://localhost:8088/cluster\r\nHDFS NameNode: http://localhost:9870/dfshealth.html#tab-overview # or 9868 (command ss -tuln)\n"})}),"\n",(0,r.jsx)(a.hr,{})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,r.jsx)(a,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,a,n)=>{n.d(a,{R:()=>t,x:()=>i});var o=n(6540);const r={},s=o.createContext(r);function t(e){const a=o.useContext(s);return o.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function i(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),o.createElement(s.Provider,{value:a},e.children)}}}]);