"use strict";(self.webpackChunkdhs_docs=self.webpackChunkdhs_docs||[]).push([[205],{8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>i});var s=r(6540);const a={},t=s.createContext(a);function o(e){const n=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),s.createElement(t.Provider,{value:n},e.children)}},9262:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>p,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"setups/spark","title":"Spark Cluster Setup","description":"---","source":"@site/docs/setups/spark.md","sourceDirName":"setups","slug":"/setups/spark","permalink":"/DHS-Documentation/docs/setups/spark","draft":false,"unlisted":false,"editUrl":"https://github.com/Prashu7487/DHS-Documentation/edit/main/docs/setups/spark.md","tags":[],"version":"current","lastUpdatedBy":"Prashu7487","lastUpdatedAt":1744625530000,"frontMatter":{},"sidebar":"docs","previous":{"title":"Hadoop Cluster Setup","permalink":"/DHS-Documentation/docs/setups/hadoop"},"next":{"title":"AWS Integration","permalink":"/DHS-Documentation/docs/setups/aws"}}');var a=r(4848),t=r(8453);const o={},i="Spark Cluster Setup",l={},c=[{value:"\ud83d\udce5 Install Spark 3.5.5 (with Hadoop 3.x)",id:"-install-spark-355-with-hadoop-3x",level:2},{value:"\ud83d\udce6 Set Environment Variables",id:"-set-environment-variables",level:2},{value:"\ud83d\udd17 Link Hadoop Configuration to Spark",id:"-link-hadoop-configuration-to-spark",level:2},{value:"\ud83d\udd27 Configure Spark",id:"-configure-spark",level:2},{value:"1. <code>spark-env.sh</code>",id:"1-spark-envsh",level:3},{value:"2. <code>spark-defaults.conf</code>",id:"2-spark-defaultsconf",level:3},{value:"\ud83e\uddf9 Clear Spark Cache (Optional)",id:"-clear-spark-cache-optional",level:2},{value:"\u2705 Verify Spark Installation",id:"-verify-spark-installation",level:2},{value:"\ud83d\ude80 Starting Spark Services",id:"-starting-spark-services",level:2},{value:"\u26a0\ufe0f Best Practices and Precautions",id:"\ufe0f-best-practices-and-precautions",level:2},{value:"\ud83c\udf10 Access web services-",id:"-access-web-services-",level:2},{value:"\ud83d\udd17 Common Spark Web UIs",id:"-common-spark-web-uis",level:3}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"spark-cluster-setup",children:"Spark Cluster Setup"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"-install-spark-355-with-hadoop-3x",children:"\ud83d\udce5 Install Spark 3.5.5 (with Hadoop 3.x)"}),"\n",(0,a.jsx)(n.p,{children:"First, choose a Spark version compatible with your Hadoop installation. For Hadoop 3.4.1 we're using Spark 3.5.5 binary:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"wget https://dlcdn.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz\r\n\r\n# make a directory for spark\r\nmkdir ~/spark\r\n\r\n# unzip binary package\r\ntar -xvzf spark-3.5.5-bin-hadoop3.tgz -C ~/spark\n"})}),"\n",(0,a.jsx)(n.h2,{id:"-set-environment-variables",children:"\ud83d\udce6 Set Environment Variables"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"nano ~/.bashrc\n"})}),"\n",(0,a.jsx)(n.p,{children:"Setup spark environment variables at the end of the .bashrc file as below and then save the bash file and close it."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Adding env vars for Spark (venv is the virtaul environment with pyspark and other dependencies installed)\r\n# wslenv is virtual env with pyspark==3.5.5\r\nexport PYSPARK_PYTHON=$HOME/wslenv/bin/python\r\nexport PYSPARK_DRIVER_PYTHON=$HOME/wslenv/bin/python\r\n\r\n# Spark environment variables\r\nexport SPARK_HOME=$HOME/spark/spark-3.5.5-bin-hadoop3\r\nexport PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\r\n\r\n# export SPARK_LOCAL_IP=$(hostname -I | awk '{print $1}')\r\n# export SPARK_MASTER_HOST=${SPARK_LOCAL_IP}\r\n# export SPARK_LOCAL_HOSTNAME=${SPARK_LOCAL_IP}\r\n\n"})}),"\n",(0,a.jsx)(n.p,{children:"Apply the changes to the current terminal session:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"source ~/.bashrc\n"})}),"\n",(0,a.jsx)(n.h2,{id:"-link-hadoop-configuration-to-spark",children:"\ud83d\udd17 Link Hadoop Configuration to Spark"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"ln -s $HADOOP_CONF_DIR/core-site.xml $SPARK_HOME/conf/\r\nln -s $HADOOP_CONF_DIR/hdfs-site.xml $SPARK_HOME/conf/\n"})}),"\n",(0,a.jsx)(n.h2,{id:"-configure-spark",children:"\ud83d\udd27 Configure Spark"}),"\n",(0,a.jsxs)(n.h3,{id:"1-spark-envsh",children:["1. ",(0,a.jsx)(n.code,{children:"spark-env.sh"})]}),"\n",(0,a.jsx)(n.p,{children:"Create and edit the spark-env.sh configuration file:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"cp $SPARK_HOME/conf/spark-env.sh.template $SPARK_HOME/conf/spark-env.sh\r\nnano $SPARK_HOME/conf/spark-env.sh\n"})}),"\n",(0,a.jsx)(n.p,{children:"Add the following configurations:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Core Paths\r\nexport SPARK_LOCAL_IP=$(hostname -I | awk '{print $1}')\r\nexport SPARK_MASTER_HOST=$(hostname -I | awk '{print $1}')\r\n#export SPARK_LOCAL_HOSTNAME=$(hostname -I | awk '{print $1}')\r\n\r\n# Hadoop Integration\r\nexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\r\nexport SPARK_DIST_CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath)\r\nexport LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:$LD_LIBRARY_PATH\r\nexport YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop\r\n\r\n#For standalone cluster Mode Settings, total memory should be less than avail (consider mry for OS too)\r\n#export SPARK_WORKER_CORES=2\r\n#export SPARK_WORKER_MEMORY=2g\r\n# export SPARK_MASTER_PORT=7077\r\n# export SPARK_MASTER_WEBUI_PORT=8080\n"})}),"\n",(0,a.jsxs)(n.h3,{id:"2-spark-defaultsconf",children:["2. ",(0,a.jsx)(n.code,{children:"spark-defaults.conf"})]}),"\n",(0,a.jsx)(n.p,{children:"Edit Spark\u2019s default configuration file:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"nano $SPARK_HOME/conf/spark-defaults.conf\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Sample configuration:\r\n",(0,a.jsx)(n.strong,{children:"Note:"})," For a single-node cluster, make sure to use your actual private IP address directly in the configuration file. Avoid using variables from ",(0,a.jsx)(n.code,{children:"~/.bashrc"})," file, as daemon processes might run in a different environment and may not be able to interpret those variables."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# HDFS as default filesystem\r\nspark.hadoop.fs.defaultFS hdfs://172.31.47.110:9000\r\n\r\n#######################################################\r\n# configurations below depends on your hardware and usecase\r\n# this is for m5.2xlarge ec2 type\r\n######################################################\r\n\r\nspark.executor.memory 6g\r\nspark.executor.cores 2\r\nspark.executor.instances 3\r\n# for single node using static allocation is better\r\nspark.dynamicAllocation.enabled false\n"})}),"\n",(0,a.jsx)(n.h2,{id:"-clear-spark-cache-optional",children:"\ud83e\uddf9 Clear Spark Cache (Optional)"}),"\n",(0,a.jsx)(n.p,{children:"If any configuration changes or JAR updates occur, clear the local YARN cache to avoid inconsistencies:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"rm -rf ~/hadoop/hadoop-3.4.1/tmp/nm-local-dir/usercache/*\n"})}),"\n",(0,a.jsx)(n.h2,{id:"-verify-spark-installation",children:"\u2705 Verify Spark Installation"}),"\n",(0,a.jsx)(n.p,{children:"Ensure Hadoop is correctly configured and running before using Spark in yarn mode."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"spark-shell\n"})}),"\n",(0,a.jsx)(n.p,{children:"Or test with:"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Note:"})," Hadoop and spark services should be running for pyspark command to work"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'pyspark --master yarn --deploy-mode client\r\n\r\n# Test-\r\nval data = Seq(("Prashant", 25), ("Spark", 10))\r\nval df = data.toDF("Name", "Age")\r\ndf.show()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"-starting-spark-services",children:"\ud83d\ude80 Starting Spark Services"}),"\n",(0,a.jsx)(n.p,{children:"Spark provides several daemons to manage distributed jobs."}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Master: Manages the cluster, tracking resource usage and worker nodes."}),"\n",(0,a.jsx)(n.li,{children:"Worker: Executes the tasks sent by the master."}),"\n",(0,a.jsx)(n.li,{children:"History Server: Provides a web UI to view completed job histories."}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Note:"})," There is a script in additional setup section using which you can start all hadoop and spark services at once.\r\nStart the services:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Start Spark Master\r\n$SPARK_HOME/sbin/start-master.sh\r\n# Start Spark Worker\r\n$SPARK_HOME/sbin/start-slave.sh spark://localhost:7077\r\n# Start Spark History Server\r\n$SPARK_HOME/sbin/start-history-server.sh\r\n\r\n# start and stop everything at once-\r\n$SPARK_HOME/sbin/start-all.sh\r\n$SPARK_HOME/sbin/stop-all.sh\n"})}),"\n",(0,a.jsx)(n.h2,{id:"\ufe0f-best-practices-and-precautions",children:"\u26a0\ufe0f Best Practices and Precautions"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Single JVM Limitation:"})," Spark supports only one session per JVM. Ensure no other Spark contexts are active by any process before initializing a new session."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Caution:"})," Ensure that all relevant hadoop and spark services are started before using FedClient's features related to these services"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Output Format:"})," When using multiple workers, Spark may write output as directories instead of single files\u2014plan accordingly."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Networking:"})," Replace localhost and 127.0.0.1 with private/public IPs in distributed environments."]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"-access-web-services-",children:"\ud83c\udf10 Access web services-"}),"\n",(0,a.jsxs)(n.p,{children:["\u26a0\ufe0f ",(0,a.jsx)(n.strong,{children:"Important:"})," Web UI Links May Vary by Port & Access"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["For a deployed cluster, replace ",(0,a.jsx)(n.code,{children:"localhost"})," with your public IP address and ensure the required ports are open in your security group."]}),"\n",(0,a.jsxs)(n.li,{children:["Alternatively, set up an SSH tunnel from your local machine to the server (see script in the ",(0,a.jsx)(n.code,{children:"additional-setup"})," section) to make the following links work properly."]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"-common-spark-web-uis",children:"\ud83d\udd17 Common Spark Web UIs"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"YARN ResourceManager"}),": ",(0,a.jsx)(n.a,{href:"http://localhost:8088/cluster",children:"http://localhost:8088/cluster"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"NodeManager"}),": ",(0,a.jsx)(n.a,{href:"http://localhost:8042/node/",children:"http://localhost:8042/node/"})]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"You can use these web UIs to monitor and debug most aspects of the system. However, for programmatic access or command-line inspection, the following commands can be useful:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Monitor YARN resource usage in real-time\r\nyarn top\r\n\r\n# View the latest 50 lines of the log\r\n# use tail -n 50 for last 50 lines of log\r\n# real-time\r\ntail -f $SPARK_HOME/logs/*.out\r\ntail -f $SPARK_HOME/logs/*Master*.out\r\ntail -f $SPARK_HOME/logs/*Worker*.out\r\n\n"})}),"\n",(0,a.jsx)(n.hr,{})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);