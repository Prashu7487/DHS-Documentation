"use strict";(self.webpackChunkdhs_docs=self.webpackChunkdhs_docs||[]).push([[27],{5673:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>l,contentTitle:()=>t,default:()=>h,frontMatter:()=>i,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"setups/aws","title":"AWS Integration","description":"---","source":"@site/docs/setups/aws.md","sourceDirName":"setups","slug":"/setups/aws","permalink":"/DHS-Documentation/docs/setups/aws","draft":false,"unlisted":false,"editUrl":"https://github.com/Prashu7487/DHS-Documentation/edit/main/docs/setups/aws.md","tags":[],"version":"current","lastUpdatedBy":"Prashu7487","lastUpdatedAt":1744625530000,"frontMatter":{},"sidebar":"docs","previous":{"title":"Spark Cluster Setup","permalink":"/DHS-Documentation/docs/setups/spark"},"next":{"title":"Additional Setup","permalink":"/DHS-Documentation/docs/setups/additional-setup"}}');var r=n(4848),o=n(8453);const i={},t="AWS Integration",l={},c=[{value:"\ud83d\udce5 Install AWS CLI",id:"-install-aws-cli",level:2},{value:"\u2705 Verify Installation",id:"-verify-installation",level:2},{value:"\ud83d\udd10 Setting AWS Credentials",id:"-setting-aws-credentials",level:2},{value:"\ud83d\udee0\ufe0f Configure AWS CLI",id:"\ufe0f-configure-aws-cli",level:2},{value:"\ud83c\udf10 Connect S3 with HDFS",id:"-connect-s3-with-hdfs",level:2},{value:"\ud83d\udce6 Install required JARs (Highly Version-Sensitive)",id:"-install-required-jars-highly-version-sensitive",level:2},{value:"\ud83d\udcda Version Compatibility and References",id:"-version-compatibility-and-references",level:3},{value:"\ud83d\udd04 Resolve Spark-Hadoop Version Conflict",id:"-resolve-spark-hadoop-version-conflict",level:2},{value:"1\ufe0f\u20e3 Backup Old hadoop specific JARs",id:"1\ufe0f\u20e3-backup-old-hadoop-specific-jars",level:3},{value:"2\ufe0f\u20e3 Copy Compatible JARs",id:"2\ufe0f\u20e3-copy-compatible-jars",level:3},{value:"\u2601\ufe0f Upload Spark JARs to HDFS",id:"\ufe0f-upload-spark-jars-to-hdfs",level:2},{value:"\u2699\ufe0f Update Spark Configuration",id:"\ufe0f-update-spark-configuration",level:2},{value:"\ud83d\udcc1 Optional: Validate Configuration Files",id:"-optional-validate-configuration-files",level:2},{value:"\u2705 \u2705 Test S3 Connectivity with HDFS",id:"--test-s3-connectivity-with-hdfs",level:2},{value:"\ud83d\udd01 Test File Transfer with distcp",id:"-test-file-transfer-with-distcp",level:2}];function d(e){const s={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(s.header,{children:(0,r.jsx)(s.h1,{id:"aws-integration",children:"AWS Integration"})}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsx)(s.h2,{id:"-install-aws-cli",children:"\ud83d\udce5 Install AWS CLI"}),"\n",(0,r.jsx)(s.p,{children:"To write data to Amazon S3 directly from HDFS using Spark, Spark must be able to access AWS credentials and configuration. Follow the steps below to set up the environment."}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-bash",children:'curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"\r\nunzip awscliv2.zip\r\nsudo ./aws/install\n'})}),"\n",(0,r.jsx)(s.h2,{id:"-verify-installation",children:"\u2705 Verify Installation"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-bash",children:"which aws\r\naws --version\n"})}),"\n",(0,r.jsx)(s.h2,{id:"-setting-aws-credentials",children:"\ud83d\udd10 Setting AWS Credentials"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsx)(s.li,{children:"Keep these credentials secret. You must have appropriate permissions to create an access key."}),"\n"]}),"\n",(0,r.jsx)(s.p,{children:"Follow these steps to create your AWS Access Key and Secret Access Key:"}),"\n",(0,r.jsxs)(s.ol,{children:["\n",(0,r.jsxs)(s.li,{children:["Log in to the ",(0,r.jsx)(s.strong,{children:(0,r.jsx)(s.a,{href:"https://console.aws.amazon.com/",children:"AWS Management Console"})}),"."]}),"\n",(0,r.jsxs)(s.li,{children:["Navigate to the ",(0,r.jsx)(s.strong,{children:"IAM (Identity and Access Management)"})," service."]}),"\n",(0,r.jsxs)(s.li,{children:["In the left sidebar, click on ",(0,r.jsx)(s.strong,{children:"Users"}),"."]}),"\n",(0,r.jsxs)(s.li,{children:["Click (not just select) your ",(0,r.jsx)(s.strong,{children:"username"})," from the list."]}),"\n",(0,r.jsxs)(s.li,{children:["Go to the ",(0,r.jsx)(s.strong,{children:"Security credentials"})," tab."]}),"\n",(0,r.jsxs)(s.li,{children:["Scroll down to the ",(0,r.jsx)(s.strong,{children:"Access keys"})," section."]}),"\n",(0,r.jsxs)(s.li,{children:["Click ",(0,r.jsx)(s.strong,{children:"Create access key"}),"."]}),"\n",(0,r.jsxs)(s.li,{children:["Select ",(0,r.jsx)(s.strong,{children:"Command Line Interface (CLI)"})," as the use case."]}),"\n",(0,r.jsxs)(s.li,{children:["Click ",(0,r.jsx)(s.strong,{children:"Next"}),", give your access key a name (label), and then click ",(0,r.jsx)(s.strong,{children:"Create access key"}),"."]}),"\n",(0,r.jsxs)(s.li,{children:["You\u2019ll now be shown:","\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:["\u2705 ",(0,r.jsx)(s.strong,{children:"Access Key ID"})]}),"\n",(0,r.jsxs)(s.li,{children:["\u2705 ",(0,r.jsx)(s.strong,{children:"Secret Access Key"})," (this is shown ",(0,r.jsx)(s.strong,{children:"only once"})," \u2014 be sure to save it securely!)"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(s.blockquote,{children:["\n",(0,r.jsxs)(s.p,{children:["\ud83d\udcbe You can download a ",(0,r.jsx)(s.code,{children:".csv"})," file containing the keys for safekeeping."]}),"\n",(0,r.jsxs)(s.p,{children:["\u26a0\ufe0f Keep these credentials ",(0,r.jsx)(s.strong,{children:"private"})," and ",(0,r.jsx)(s.strong,{children:"secure"}),"."]}),"\n"]}),"\n",(0,r.jsx)(s.h2,{id:"\ufe0f-configure-aws-cli",children:"\ud83d\udee0\ufe0f Configure AWS CLI"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-bash",children:"aws configure\n"})}),"\n",(0,r.jsx)(s.p,{children:"Provide the following details:"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsx)(s.li,{children:"Access Key ID: your-access-key"}),"\n",(0,r.jsx)(s.li,{children:"Secret Key: your-secret-key"}),"\n",(0,r.jsx)(s.li,{children:"Region: your-region (e.g., ap-south-1)"}),"\n",(0,r.jsx)(s.li,{children:"Output: json"}),"\n"]}),"\n",(0,r.jsx)(s.h2,{id:"-connect-s3-with-hdfs",children:"\ud83c\udf10 Connect S3 with HDFS"}),"\n",(0,r.jsx)(s.p,{children:"update core-site.xml"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-bash",children:"cd ~/hadoop/hadoop-3.4.1/etc/hadoop\r\nnano core-site.xml\n"})}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Append"}),' the following properties inside the "configuration" tag:']}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-xml",children:"<configuration>\r\n    \x3c!-- for AWS and distcp setup --\x3e\r\n    <property>\r\n        <name>fs.s3a.impl</name>\r\n        <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>\r\n    </property>\r\n\r\n        <property>\r\n        <name>fs.s3a.aws.credentials.provider</name>\r\n        <value>software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider</value>\r\n        \x3c!-- value will be com.amazonaws.auth.DefaultAWSCredentialsProviderChain for SDK v2--\x3e\r\n    </property>\r\n\r\n    <property>\r\n        <name>fs.s3a.fast.upload</name>\r\n        <value>true</value>\r\n    </property>\r\n\r\n    <property>\r\n        <name>fs.s3a.connection.maximum</name>\r\n        <value>100</value>\r\n    </property>\r\n</configuration>\n"})}),"\n",(0,r.jsx)(s.h2,{id:"-install-required-jars-highly-version-sensitive",children:"\ud83d\udce6 Install required JARs (Highly Version-Sensitive)"}),"\n",(0,r.jsx)(s.p,{children:"Install JARs for Hadoop + Spark to enable S3 communication. These are version-sensitive."}),"\n",(0,r.jsx)(s.h3,{id:"-version-compatibility-and-references",children:"\ud83d\udcda Version Compatibility and References"}),"\n",(0,r.jsxs)(s.p,{children:["\ud83e\udde9 Use ",(0,r.jsx)(s.a,{href:"https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-aws/3.4.1",children:"Maven Central"})," to find and match the correct versions of JARs for your Hadoop and Spark setup."]}),"\n",(0,r.jsx)(s.p,{children:"Additional helpful references:"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:["\ud83d\udcd6 ",(0,r.jsx)(s.a,{href:"https://hadoop.apache.org/docs/r3.4.0/hadoop-aws/tools/hadoop-aws/index.html#General_S3A_Client_configuration",children:"Hadoop AWS Documentation"})," \u2013 General configuration for using AWS S3 with Hadoop."]}),"\n",(0,r.jsxs)(s.li,{children:["\ud83d\udd27 ",(0,r.jsx)(s.a,{href:"https://hadoop.apache.org/docs/r3.4.1/hadoop-aws/tools/hadoop-aws/troubleshooting_s3a.html",children:"S3A Troubleshooting Guide"})," \u2013 Useful for debugging common S3 integration issues."]}),"\n",(0,r.jsxs)(s.li,{children:["\ud83d\udcac ",(0,r.jsx)(s.a,{href:"https://stackoverflow.com/questions/44411493/java-lang-noclassdeffounderror-org-apache-hadoop-fs-storagestatistics",children:"Stack Overflow: ClassNotFoundError for StorageStatistics"})," \u2013 Discussion on common JAR mismatch or missing class issues."]}),"\n"]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-bash",children:'\r\nwget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.4.1/hadoop-aws-3.4.1.jar -P $HADOOP_HOME/share/hadoop/common/lib/\r\n\r\nwget https://repo1.maven.org/maven2/com/google/guava/guava/27.0-jre/guava-27.0-jre.jar -P $HADOOP_HOME/share/hadoop/common/lib/\r\n\r\n## install only one of the next 2 (either SDKv1 jar or SDKv2 jar, and be consistent with it in furthur commands)\r\n\r\n# For AWS SDKv1 use this and "com.amazonaws.auth.DefaultAWSCredentialsProviderChain" class as credential provider\r\nwget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar -P $HADOOP_HOME/share/hadoop/common/lib/\r\n\r\n# OR, for  AWS SDKv2 (hadoop 3.4+)  use this and "software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider" as credential provider\r\nwget https://repo1.maven.org/maven2/software/amazon/awssdk/bundle/2.24.6/bundle-2.24.6.jar -P $HADOOP_HOME/share/hadoop/common/lib/\n'})}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Note:"}),' If the SDK v2 credential class doesn\u2019t work, fallback to using "com.amazonaws.auth.DefaultAWSCredentialsProviderChain".']}),"\n",(0,r.jsx)(s.h2,{id:"-resolve-spark-hadoop-version-conflict",children:"\ud83d\udd04 Resolve Spark-Hadoop Version Conflict"}),"\n",(0,r.jsx)(s.p,{children:"Downloaded Spark may contain incompatible Hadoop JARs. To resolve this:"}),"\n",(0,r.jsx)(s.h3,{id:"1\ufe0f\u20e3-backup-old-hadoop-specific-jars",children:"1\ufe0f\u20e3 Backup Old hadoop specific JARs"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-bash",children:"mkdir -p ~/spark-hadoop-jars-backup\r\nmv $SPARK_HOME/jars/hadoop-* ~/spark-hadoop-jars-backup\n"})}),"\n",(0,r.jsx)(s.h3,{id:"2\ufe0f\u20e3-copy-compatible-jars",children:"2\ufe0f\u20e3 Copy Compatible JARs"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-bash",children:"# Copy Hadoop 3.4.1 JARs from your Hadoop installation to spark jars\r\nHADOOP_LIBS_DIR=$HADOOP_HOME/share/hadoop\r\ncp $HADOOP_LIBS_DIR/common/hadoop-common-3.4.1.jar $SPARK_HOME/jars/\r\ncp $HADOOP_LIBS_DIR/common/lib/hadoop-aws-3.4.1.jar $SPARK_HOME/jars/\r\ncp $HADOOP_LIBS_DIR/common/lib/guava-27.0-jre.jar $SPARK_HOME/jars/\r\ncp $HADOOP_LIBS_DIR/common/lib/bundle-2.24.6.jar $SPARK_HOME/jars/ (or aws-java-sdk-bundle-1.12.262.jar as per prev step)\n"})}),"\n",(0,r.jsx)(s.h2,{id:"\ufe0f-upload-spark-jars-to-hdfs",children:"\u2601\ufe0f Upload Spark JARs to HDFS"}),"\n",(0,r.jsx)(s.p,{children:"this avoids Spark warnings during job submission:"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-bash",children:'# Upload Spark JARs to HDFS (hadoop services should be running for this commandto work)\r\nhdfs dfs -mkdir -p /spark-jars\r\nhdfs dfs -put $SPARK_HOME/jars/* /spark-jars/\r\n\r\n# Add the path to spark-defaults.conf by echo command\r\necho "spark.yarn.jars hdfs:///spark-jars/*" >> $SPARK_HOME/conf/spark-defaults.conf\n'})}),"\n",(0,r.jsx)(s.h2,{id:"\ufe0f-update-spark-configuration",children:"\u2699\ufe0f Update Spark Configuration"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-bash",children:"nano $SPARK_HOME/conf/spark-defaults.conf\n"})}),"\n",(0,r.jsx)(s.p,{children:"Append the following:"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-bash",children:"# HDFS as default filesystem\r\nspark.hadoop.fs.defaultFS hdfs://0.0.0.0:9000\r\n\r\n# S3A configuration\r\n# spark.hadoop.fs.s3a.impl org.apache.hadoop.fs.s3a.S3AFileSystem\r\nspark.hadoop.fs.s3a.endpoint s3.amazonaws.com\r\n\r\n# Performance optimizations (optional)\r\nspark.hadoop.fs.s3a.connection.maximum 1000\r\nspark.hadoop.fs.s3a.fast.upload true\r\n\r\n# for sdk v2: spark.hadoop.fs.s3a.aws.credentials.provider software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider\r\nspark.hadoop.fs.s3a.aws.credentials.provider com.amazonaws.auth.DefaultAWSCredentialsProviderChain\r\n\r\nspark.hadoop.fs.s3a.impl org.apache.hadoop.fs.s3a.S3AFileSystem\r\nspark.yarn.jars hdfs:///spark-jars/*\r\n\n"})}),"\n",(0,r.jsx)(s.h2,{id:"-optional-validate-configuration-files",children:"\ud83d\udcc1 Optional: Validate Configuration Files"}),"\n",(0,r.jsxs)(s.p,{children:["Make sure ",(0,r.jsx)(s.code,{children:"core-site.xml"})," and ",(0,r.jsx)(s.code,{children:"hdfs-site.xml"})," are available in Spark\u2019s config directory:\r\n(these will be avail because of linking done in spark setup)"]}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-bash",children:"ls $SPARK_HOME/conf\n"})}),"\n",(0,r.jsx)(s.h2,{id:"--test-s3-connectivity-with-hdfs",children:"\u2705 \u2705 Test S3 Connectivity with HDFS"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-bash",children:"hadoop fs -ls s3a://your-bucket-name/\n"})}),"\n",(0,r.jsx)(s.h2,{id:"-test-file-transfer-with-distcp",children:"\ud83d\udd01 Test File Transfer with distcp"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-bash",children:"hadoop distcp hdfs:///user/username/path/ s3a://your-bucket-name/path/\n"})}),"\n",(0,r.jsx)(s.p,{children:"\u2705 You're now ready to use Spark to read/write data directly from Amazon S3."})]})}function h(e={}){const{wrapper:s}={...(0,o.R)(),...e.components};return s?(0,r.jsx)(s,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,s,n)=>{n.d(s,{R:()=>i,x:()=>t});var a=n(6540);const r={},o=a.createContext(r);function i(e){const s=a.useContext(o);return a.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function t(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),a.createElement(o.Provider,{value:s},e.children)}}}]);