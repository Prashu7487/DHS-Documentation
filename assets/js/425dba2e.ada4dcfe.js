"use strict";(self.webpackChunkdhs_docs=self.webpackChunkdhs_docs||[]).push([[886],{7053:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>l,contentTitle:()=>d,default:()=>c,frontMatter:()=>t,metadata:()=>a,toc:()=>p});const a=JSON.parse('{"id":"setups/hadoop","title":"Hadoop Cluster Setup","description":"---","source":"@site/docs/setups/hadoop.md","sourceDirName":"setups","slug":"/setups/hadoop","permalink":"/dhs-docs/docs/setups/hadoop","draft":false,"unlisted":false,"editUrl":"https://github.com/Prashu7487/DHS-Documentation/edit/main/docs/setups/hadoop.md","tags":[],"version":"current","lastUpdatedBy":"Prashu7487","lastUpdatedAt":1744379635000,"frontMatter":{},"sidebar":"docs","previous":{"title":"Installation & Dependencies","permalink":"/dhs-docs/docs/installation/dependencies"},"next":{"title":"Apache Spark Setup","permalink":"/dhs-docs/docs/setups/spark"}}');var s=n(4848),o=n(8453);const t={},d="Hadoop Cluster Setup",l={},p=[{value:"\ud83e\uddf0 Prerequisites",id:"-prerequisites",level:2},{value:"\ud83d\udce5 Install Hadoop 3.4.1",id:"-install-hadoop-341",level:2},{value:"\ud83c\udf0d Set Environment Variables",id:"-set-environment-variables",level:2},{value:"\ud83d\udd11 Configure SSH",id:"-configure-ssh",level:2},{value:"\ud83d\udd27 Configure Hadoop",id:"-configure-hadoop",level:2},{value:"\ud83d\uddc2\ufe0f Create Directories",id:"\ufe0f-create-directories",level:2},{value:"\u2699\ufe0f Configure Core Site",id:"\ufe0f-configure-core-site",level:2},{value:"\ud83d\udcc2 Create HDFS Folders",id:"-create-hdfs-folders",level:2},{value:"\ud83d\udccc Notes for HDFS Setup",id:"-notes-for-hdfs-setup",level:2}];function i(e){const r={code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(r.header,{children:(0,s.jsx)(r.h1,{id:"hadoop-cluster-setup",children:"Hadoop Cluster Setup"})}),"\n",(0,s.jsx)(r.hr,{}),"\n",(0,s.jsx)(r.h2,{id:"-prerequisites",children:"\ud83e\uddf0 Prerequisites"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsx)(r.li,{children:"Java 8 OpenJDK"}),"\n",(0,s.jsx)(r.li,{children:"SSH configured between nodes"}),"\n",(0,s.jsx)(r.li,{children:"Proper user permissions and folders created"}),"\n"]}),"\n",(0,s.jsx)(r.hr,{}),"\n",(0,s.jsx)(r.h2,{id:"-install-hadoop-341",children:"\ud83d\udce5 Install Hadoop 3.4.1"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-bash",children:"wget https://archive.apache.org/dist/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz\r\ntar -xzvf hadoop-3.4.1.tar.gz\n"})}),"\n",(0,s.jsx)(r.h2,{id:"-set-environment-variables",children:"\ud83c\udf0d Set Environment Variables"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-bash",children:"export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\r\nexport HADOOP_HOME=~/hadoop/hadoop-3.4.1\r\nexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\n"})}),"\n",(0,s.jsx)(r.h2,{id:"-configure-ssh",children:"\ud83d\udd11 Configure SSH"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-bash",children:"ssh-keygen -t rsa\r\nssh-copy-id user@localhost\n"})}),"\n",(0,s.jsx)(r.h2,{id:"-configure-hadoop",children:"\ud83d\udd27 Configure Hadoop"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-bash",children:"cd $HADOOP_HOME/etc/hadoop\r\ncp mapred-site.xml.template mapred-site.xml\r\ncp yarn-site.xml.template yarn-site.xml\n"})}),"\n",(0,s.jsx)(r.h2,{id:"\ufe0f-create-directories",children:"\ud83d\uddc2\ufe0f Create Directories"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-bash",children:"mkdir -p $HADOOP_HOME/logs\r\nmkdir -p $HADOOP_HOME/tmp\r\nmkdir -p $HADOOP_HOME/data\r\nmkdir -p $HADOOP_HOME/data/hdfs/namenode\r\nmkdir -p $HADOOP_HOME/data/hdfs/datanode\n"})}),"\n",(0,s.jsx)(r.h2,{id:"\ufe0f-configure-core-site",children:"\u2699\ufe0f Configure Core Site"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-xml",children:"<configuration>\r\n    <property>\r\n        <name>fs.defaultFS</name>\r\n        <value>hdfs://localhost:9000</value>\r\n    </property>\r\n    <property>\r\n        <name>hadoop.tmp.dir</name>\r\n        <value>/tmp/hadoop-${user.name}</value>\r\n    </property>\r\n    <property>\r\n        <name>io.file.buffer.size</name>\r\n        <value>131072</value>\r\n    </property>\r\n    <property>\r\n        <name>fs.default.name</name>\r\n        <value>hdfs://localhost:9000</value>\r\n    </property>\r\n    <property>\r\n        <name>fs.checkpoint.period</name>\r\n        <value>600</value>\r\n    </property>\r\n    <property>\r\n        <name>fs.checkpoint.size</name>\r\n        <value>67108864</value>\r\n    </property>\r\n    <property>\r\n        <name>fs.defaultFS</name>\r\n        <value>hdfs://localhost:9000</value>\r\n    </property>\r\n    <property>\r\n        <name>dfs.replication</name>\r\n        <value>1</value>\r\n    </property>\r\n    <property>\r\n        <name>dfs.namenode.name.dir</name>\r\n        <value>file:///home/user/hadoop/data/hdfs/namenode</value>\r\n    </property>\r\n    <property>\r\n        <name>dfs.datanode.data.dir</name>\r\n        <value>file:///home/user/hadoop/data/hdfs/datanode</value>\r\n    </property>\r\n    <property>\r\n        <name>dfs.namenode.checkpoint.dir</name>\r\n        <value>file:///home/user/hadoop/data/hdfs/namenode</value>\r\n    </property>\r\n    <property>\r\n        <name>dfs.namenode.secondary.http-address</name>\r\n        <value>localhost:50090</value>\r\n    </property>\r\n    <property>\r\n        <name>dfs.namenode.secondary.http-address</name>\r\n        <value>localhost:50090</value>\r\n    </property>\r\n    <property>\r\n        <name>dfs.namenode.heartbeat.interval</name>\r\n        <value>3</value>\r\n    </property>\r\n    <property>\r\n        <name>dfs.namenode.heartbeat.recheck-interval</name>\r\n        <value>3000</value>\r\n    </property>\r\n    <property>\r\n        <name>dfs.namenode.handler.count</name>\r\n        <value>10</value>\r\n    </property>\r\n    <property>\r\n        <name>dfs.datanode.handler.count</name>\r\n        <value>10</value>\r\n    </property>\r\n    <property>\r\n        <name>dfs.datanode.max.transfer.threads</name>\r\n        <value>4096</value>\r\n    </property>\r\n<configuration>\n"})}),"\n",(0,s.jsx)(r.h2,{id:"-create-hdfs-folders",children:"\ud83d\udcc2 Create HDFS Folders"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-bash",children:"hdfs dfs -mkdir /user\r\nhdfs dfs -mkdir /user/cdis/{tmpuploads,uploads,processed}\n"})}),"\n",(0,s.jsx)(r.h2,{id:"-notes-for-hdfs-setup",children:"\ud83d\udccc Notes for HDFS Setup"}),"\n",(0,s.jsx)(r.p,{children:"Enable WebHDFS in hdfs-site.xml"}),"\n",(0,s.jsx)(r.p,{children:"Format NameNode only after cleaning data directories"}),"\n",(0,s.jsx)(r.p,{children:"Ensure correct file paths and permissions"}),"\n",(0,s.jsx)(r.p,{children:"Avoid external interference with HDFS to prevent metadata mismatch"})]})}function c(e={}){const{wrapper:r}={...(0,o.R)(),...e.components};return r?(0,s.jsx)(r,{...e,children:(0,s.jsx)(i,{...e})}):i(e)}},8453:(e,r,n)=>{n.d(r,{R:()=>t,x:()=>d});var a=n(6540);const s={},o=a.createContext(s);function t(e){const r=a.useContext(o);return a.useMemo((function(){return"function"==typeof e?e(r):{...r,...e}}),[r,e])}function d(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),a.createElement(o.Provider,{value:r},e.children)}}}]);