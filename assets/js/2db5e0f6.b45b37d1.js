"use strict";(self.webpackChunkdhs_docs=self.webpackChunkdhs_docs||[]).push([[205],{8453:(e,s,n)=>{n.d(s,{R:()=>o,x:()=>p});var r=n(6540);const a={},t=r.createContext(a);function o(e){const s=r.useContext(t);return r.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function p(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),r.createElement(t.Provider,{value:s},e.children)}},9262:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>l,contentTitle:()=>p,default:()=>d,frontMatter:()=>o,metadata:()=>r,toc:()=>i});const r=JSON.parse('{"id":"setups/spark","title":"Apache Spark Setup","description":"---","source":"@site/docs/setups/spark.md","sourceDirName":"setups","slug":"/setups/spark","permalink":"/DHS-Documentation/docs/setups/spark","draft":false,"unlisted":false,"editUrl":"https://github.com/Prashu7487/DHS-Documentation/edit/main/docs/setups/spark.md","tags":[],"version":"current","lastUpdatedBy":"Prashu7487","lastUpdatedAt":1744379635000,"frontMatter":{},"sidebar":"docs","previous":{"title":"Hadoop Cluster Setup","permalink":"/DHS-Documentation/docs/setups/hadoop"},"next":{"title":"AWS Integration","permalink":"/DHS-Documentation/docs/setups/aws"}}');var a=n(4848),t=n(8453);const o={},p="Apache Spark Setup",l={},i=[{value:"\ud83d\udce5 Install Spark 3.5.5 (with Hadoop 3.x)",id:"-install-spark-355-with-hadoop-3x",level:2},{value:"\ud83d\udce6 Set Environment Variables",id:"-set-environment-variables",level:2},{value:"Link Hadoop to Spark",id:"link-hadoop-to-spark",level:2},{value:"\ud83d\udd0c Spark Service Management",id:"-spark-service-management",level:2},{value:"\u2699\ufe0f Configure spark-env.sh",id:"\ufe0f-configure-spark-envsh",level:2},{value:"\ud83d\udcc4 spark-defaults.conf",id:"-spark-defaultsconf",level:2},{value:"\ud83e\udde0 Notes on Spark Usage",id:"-notes-on-spark-usage",level:2}];function c(e){const s={code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",p:"p",pre:"pre",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(s.header,{children:(0,a.jsx)(s.h1,{id:"apache-spark-setup",children:"Apache Spark Setup"})}),"\n",(0,a.jsx)(s.hr,{}),"\n",(0,a.jsx)(s.h2,{id:"-install-spark-355-with-hadoop-3x",children:"\ud83d\udce5 Install Spark 3.5.5 (with Hadoop 3.x)"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-bash",children:"wget https://dlcdn.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz\r\nmkdir ~/spark\r\ntar -xvzf spark-3.5.5-bin-hadoop3.tgz -C ~/spark\n"})}),"\n",(0,a.jsx)(s.h2,{id:"-set-environment-variables",children:"\ud83d\udce6 Set Environment Variables"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-bash",children:"export SPARK_HOME=~/spark/spark-3.5.5-bin-hadoop3\r\nexport PATH=$SPARK_HOME/bin:$PATH\r\nexport PYSPARK_PYTHON=~/wslenv/bin/python\r\nexport PYSPARK_DRIVER_PYTHON=~/wslenv/bin/python\n"})}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-bash",children:"source ~/.bashrc\n"})}),"\n",(0,a.jsx)(s.h2,{id:"link-hadoop-to-spark",children:"Link Hadoop to Spark"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-bash",children:"ln -s $HADOOP_CONF_DIR/core-site.xml $SPARK_HOME/conf/\r\nln -s $HADOOP_CONF_DIR/hdfs-site.xml $SPARK_HOME/conf/\n"})}),"\n",(0,a.jsx)(s.h2,{id:"-spark-service-management",children:"\ud83d\udd0c Spark Service Management"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-bash",children:"# Start Spark Master\r\n$SPARK_HOME/sbin/start-master.sh\r\n# Start Spark Worker\r\n$SPARK_HOME/sbin/start-slave.sh spark://localhost:7077\r\n# Start Spark History Server\r\n$SPARK_HOME/sbin/start-history-server.sh\r\n\r\n# start and stop everything at once-\r\n$SPARK_HOME/sbin/start-all.sh\r\n$SPARK_HOME/sbin/stop-all.sh\r\n\n"})}),"\n",(0,a.jsx)(s.h2,{id:"\ufe0f-configure-spark-envsh",children:"\u2699\ufe0f Configure spark-env.sh"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-bash",children:"cp $SPARK_HOME/conf/spark-env.sh.template $SPARK_HOME/conf/spark-env.sh\r\n\r\n# then append\r\nexport HADOOP_CONF_DIR=$HADOOP_CONF_DIR\r\nexport YARN_CONF_DIR=$YARN_CONF_DIR\r\nexport SPARK_LOCAL_IP=127.0.0.1\r\nexport SPARK_MASTER_HOST=localhost\r\n\n"})}),"\n",(0,a.jsx)(s.h2,{id:"-spark-defaultsconf",children:"\ud83d\udcc4 spark-defaults.conf"}),"\n",(0,a.jsx)(s.pre,{children:(0,a.jsx)(s.code,{className:"language-bash",children:"spark.master yarn\r\nspark.submit.deployMode client\r\nspark.hadoop.fs.defaultFS hdfs://localhost:9000\n"})}),"\n",(0,a.jsx)(s.h2,{id:"-notes-on-spark-usage",children:"\ud83e\udde0 Notes on Spark Usage"}),"\n",(0,a.jsx)(s.p,{children:"Only one SparkSession per JVM"}),"\n",(0,a.jsx)(s.p,{children:"Do not start jobs before Hadoop & Spark are fully up"}),"\n",(0,a.jsx)(s.p,{children:"Spark may write outputs as folders if using multiple workers"})]})}function d(e={}){const{wrapper:s}={...(0,t.R)(),...e.components};return s?(0,a.jsx)(s,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}}}]);