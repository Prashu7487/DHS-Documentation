"use strict";(self.webpackChunkdhs_docs=self.webpackChunkdhs_docs||[]).push([[205],{8453:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>o});var s=r(6540);const a={},t=s.createContext(a);function i(e){const n=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),s.createElement(t.Provider,{value:n},e.children)}},9262:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"setups/spark","title":"Spark Cluster Setup","description":"---","source":"@site/docs/setups/spark.md","sourceDirName":"setups","slug":"/setups/spark","permalink":"/DHS-Documentation/docs/setups/spark","draft":false,"unlisted":false,"editUrl":"https://github.com/Prashu7487/DHS-Documentation/edit/main/docs/setups/spark.md","tags":[],"version":"current","lastUpdatedBy":"Prashu7487","lastUpdatedAt":1744625530000,"frontMatter":{},"sidebar":"docs","previous":{"title":"Hadoop Cluster Setup","permalink":"/DHS-Documentation/docs/setups/hadoop"},"next":{"title":"AWS Integration","permalink":"/DHS-Documentation/docs/setups/aws"}}');var a=r(4848),t=r(8453);const i={},o="Spark Cluster Setup",l={},c=[{value:"\ud83d\udce5 Install Spark 3.5.5 (with Hadoop 3.x)",id:"-install-spark-355-with-hadoop-3x",level:2},{value:"\ud83d\udce6 Set Environment Variables",id:"-set-environment-variables",level:2},{value:"\ud83d\udd17 Link Hadoop Configuration to Spark",id:"-link-hadoop-configuration-to-spark",level:2},{value:"\ud83d\udd27 Configure Spark",id:"-configure-spark",level:2},{value:"1. <code>spark-env.sh</code>",id:"1-spark-envsh",level:3},{value:"2. <code>spark-defaults.conf</code>",id:"2-spark-defaultsconf",level:3},{value:"\ud83e\uddf9 Clear Spark Cache (Optional)",id:"-clear-spark-cache-optional",level:2},{value:"\u2705 Verify Spark Installation",id:"-verify-spark-installation",level:2},{value:"\ud83d\ude80 Starting Spark Services",id:"-starting-spark-services",level:2},{value:"\u26a0\ufe0f Best Practices and Precautions",id:"\ufe0f-best-practices-and-precautions",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"spark-cluster-setup",children:"Spark Cluster Setup"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"-install-spark-355-with-hadoop-3x",children:"\ud83d\udce5 Install Spark 3.5.5 (with Hadoop 3.x)"}),"\n",(0,a.jsx)(n.p,{children:"First, choose a Spark version compatible with your Hadoop installation. For Hadoop 3.4.1 we're using Spark 3.5.5 binary:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"wget https://dlcdn.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz\r\n\r\n# make a directory for spark\r\nmkdir ~/spark\r\n\r\n# unzip binary package\r\ntar -xvzf spark-3.5.5-bin-hadoop3.tgz -C ~/spark\n"})}),"\n",(0,a.jsx)(n.h2,{id:"-set-environment-variables",children:"\ud83d\udce6 Set Environment Variables"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"nano ~/.bashrc\n"})}),"\n",(0,a.jsx)(n.p,{children:"Setup spark environment variables at the end of the .bashrc file as below and then save the bash file and close it."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Adding env vars for Spark (venv is the virtaul environment with pyspark and other dependencies installed)\r\nexport PYSPARK_PYTHON=~/venv/bin/python\r\nexport PYSPARK_DRIVER_PYTHON=~/venv/bin/python\r\n\r\n# Spark environment variables\r\nexport SPARK_HOME=$HOME/spark/spark-3.5.5-bin-hadoop3\r\nexport PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n"})}),"\n",(0,a.jsx)(n.p,{children:"Apply the changes to the current terminal session:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"source ~/.bashrc\n"})}),"\n",(0,a.jsx)(n.h2,{id:"-link-hadoop-configuration-to-spark",children:"\ud83d\udd17 Link Hadoop Configuration to Spark"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"ln -s $HADOOP_CONF_DIR/core-site.xml $SPARK_HOME/conf/\r\nln -s $HADOOP_CONF_DIR/hdfs-site.xml $SPARK_HOME/conf/\n"})}),"\n",(0,a.jsx)(n.h2,{id:"-configure-spark",children:"\ud83d\udd27 Configure Spark"}),"\n",(0,a.jsxs)(n.h3,{id:"1-spark-envsh",children:["1. ",(0,a.jsx)(n.code,{children:"spark-env.sh"})]}),"\n",(0,a.jsx)(n.p,{children:"Create and edit the spark-env.sh configuration file:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"cp $SPARK_HOME/conf/spark-env.sh.template $SPARK_HOME/conf/spark-env.sh\r\nnano $SPARK_HOME/conf/spark-env.sh\n"})}),"\n",(0,a.jsx)(n.p,{children:"Add the following configurations:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\r\nexport LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:$LD_LIBRARY_PATH  # Native Hadoop libs\r\n\r\n# Include Hadoop's classpath to avoid version conflicts later\r\nexport SPARK_DIST_CLASSPATH=$(hadoop classpath)\r\nexport YARN_CONF_DIR=$HADOOP_CONF_DIR\r\n\r\n# private IP (hostname -I)\r\nexport SPARK_LOCAL_IP=127.0.0.1\r\nexport SPARK_MASTER_HOST=localhost\n"})}),"\n",(0,a.jsxs)(n.h3,{id:"2-spark-defaultsconf",children:["2. ",(0,a.jsx)(n.code,{children:"spark-defaults.conf"})]}),"\n",(0,a.jsx)(n.p,{children:"Edit Spark\u2019s default configuration file:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"nano $SPARK_HOME/conf/spark-defaults.conf\n"})}),"\n",(0,a.jsx)(n.p,{children:"Sample configuration:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# spark master\r\nexport SPARK_MASTER_HOST=localhost # or (0.0.0.0) to allow connection from everywhere\r\nexport SPARK_LOCAL_IP=127.0.0.1  # or (0.0.0.0) to allow connection from everywhere\r\n\r\n# HDFS as default filesystem\r\nspark.hadoop.fs.defaultFS hdfs://0.0.0.0:9000\n"})}),"\n",(0,a.jsx)(n.h2,{id:"-clear-spark-cache-optional",children:"\ud83e\uddf9 Clear Spark Cache (Optional)"}),"\n",(0,a.jsx)(n.p,{children:"If any configuration changes or JAR updates occur, clear the local YARN cache to avoid inconsistencies:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"rm -rf ~/hadoop/hadoop-3.4.1/tmp/nm-local-dir/usercache/*\n"})}),"\n",(0,a.jsx)(n.h2,{id:"-verify-spark-installation",children:"\u2705 Verify Spark Installation"}),"\n",(0,a.jsxs)(n.p,{children:["Ensure Hadoop is correctly configured and running before using Spark in yarn mode. Start a PySpark session.\r\n",(0,a.jsx)(n.strong,{children:"Note:"})," Hadoop and spark services should be started for pyspark command to work"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'pyspark --master yarn --deploy-mode client\r\n\r\n# Test-\r\nval data = Seq(("Prashant", 25), ("Spark", 10))\r\nval df = data.toDF("Name", "Age")\r\ndf.show()\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Or test with:\r\n",(0,a.jsx)(n.strong,{children:"Note:"})," Spark Session will be created even if spark and hadoop services are not running"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"spark-sehll\n"})}),"\n",(0,a.jsx)(n.h2,{id:"-starting-spark-services",children:"\ud83d\ude80 Starting Spark Services"}),"\n",(0,a.jsx)(n.p,{children:"Spark provides several daemons to manage distributed jobs."}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Master: Manages the cluster, tracking resource usage and worker nodes."}),"\n",(0,a.jsx)(n.li,{children:"Worker: Executes the tasks sent by the master."}),"\n",(0,a.jsx)(n.li,{children:"History Server: Provides a web UI to view completed job histories."}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Note:"})," There is a script in additional setup section using which you can start all hadoop and spark services at once.\r\nStart the services:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Start Spark Master\r\n$SPARK_HOME/sbin/start-master.sh\r\n# Start Spark Worker\r\n$SPARK_HOME/sbin/start-slave.sh spark://localhost:7077\r\n# Start Spark History Server\r\n$SPARK_HOME/sbin/start-history-server.sh\r\n\r\n# start and stop everything at once-\r\n$SPARK_HOME/sbin/start-all.sh\r\n$SPARK_HOME/sbin/stop-all.sh\n"})}),"\n",(0,a.jsx)(n.h2,{id:"\ufe0f-best-practices-and-precautions",children:"\u26a0\ufe0f Best Practices and Precautions"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Single JVM Limitation:"})," Spark supports only one session per JVM. Ensure no other Spark contexts are active by any process before initializing a new session."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Caution:"})," Ensure that all relevant hadoop and spark services are started before using FedClient's features related to these services"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Output Format:"})," When using multiple workers, Spark may write output as directories instead of single files\u2014plan accordingly."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Networking:"})," Replace localhost and 127.0.0.1 with private/public IPs in distributed environments."]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);